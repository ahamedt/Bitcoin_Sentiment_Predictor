# -*- coding: utf-8 -*-
"""CIS 545 Final Project Modeling Notebook

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i5m7vuBVR-Zryc_C2q9AJLtAo7PMIUgm

# **CIS 545 Final Project Modeling Notebook**
## **Tahmid Ahamed, Ronak Bhatia, Vikramaditya Dugar**

Contained below is all of the code made for the models we used for analyzing the datasets.

# **Imports**
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
from google.colab import files
import seaborn as sns
import matplotlib.pyplot as plt
# %matplotlib inline
from keras.layers import Dense, LSTM
from keras.models import Sequential
from keras.optimizers import Adam
from keras.callbacks import EarlyStopping
from keras.utils import np_utils
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from keras.callbacks import History
from statsmodels.tsa.arima_model import ARIMA
import statsmodels.api
from math import sqrt

#This is the Sentiment Dataset
final_df = pd.read_csv('bitcoin_final_csv.csv').drop(columns = ['timestamp', 'Unnamed: 0'])
final_df

#This is the non sentiment Dataset
corrected = pd.read_csv('bitcoin_tweet_volume_new.csv').drop(columns = ['Unnamed: 0']).dropna()
corrected

"""# **Linear Regression (w/ and without Sentiment)**"""

w_sentiment_lr = LinearRegression()
X_w_sentiment = final_df.drop(columns = ['Timestamp', 'Weighted_Price'])
y_w_sentiment = final_df['Weighted_Price']
x_train, x_test, y_train, y_test = train_test_split(X_w_sentiment, y_w_sentiment, test_size = 0.33)
SCALER = StandardScaler()
SCALER.fit(x_train)
x_train = SCALER.transform(x_train)
x_test = SCALER.transform(x_test)
w_sentiment_lr.fit(x_train, y_train)
preds = w_sentiment_lr.predict(x_test)
print('The sentiment r2 score is '+ str(r2_score(y_test, preds)))
print('The sentiment MSE is ' + str(mean_squared_error(y_test, preds)))


wo_sentiment_lr = LinearRegression()
X_wo_sentiment = corrected.drop(columns = ['Timestamp', 'Weighted_Price'])
y_wo_sentiment = corrected['Weighted_Price']
x_train, x_test, y_train, y_test = train_test_split(X_wo_sentiment, y_wo_sentiment, test_size = 0.33)
SCALER = StandardScaler()
SCALER.fit(x_train)
x_train = SCALER.transform(x_train)
x_test = SCALER.transform(x_test)
wo_sentiment_lr.fit(x_train, y_train)
preds = wo_sentiment_lr.predict(x_test)
print('The non sentiment r2 score is ' + str(r2_score(y_test, preds)))
print('The non sentiment MSE is ' + str(mean_squared_error(y_test, preds)))

"""# **ARIMA**"""

### ARIMA ###
amodel_df = corrected.copy()
amodel = ARIMA(amodel_df[['Weighted_Price']], order = (1,1,1))
amodel_results = amodel.fit(disp = -1)
cumulative_series = pd.Series(amodel_results.fittedvalues, copy = True)
cum_sum = cumulative_series.cumsum()
plt.figure(figsize = (15,10))
plt.plot(cum_sum, label = 'ARIMA Prediction')
plt.plot(amodel_df['Weighted_Price'], label = 'Weighted Price')
plt.legend()
plt.xlabel('Time')
plt.ylabel('Price')
plt.title('Real Weighted Price vs ARIMA Forecasted Price')
print(sqrt(mean_squared_error(amodel_df['Weighted_Price'][1:], cum_sum)))

updated_model = statsmodels.api.tsa.statespace.SARIMAX(amodel_df['Weighted_Price'], order = (1,2,1), seasonal_order=(1,1,2,1))
second_amodel = updated_model.fit(disp = -1)

huh_df = amodel_df.copy()
huh_df['forecast'] = second_amodel.predict(start = 1000, end = 3000, dynamic = True)
plt.plot(amodel_df['Weighted_Price'], label = 'Weighted Price')
plt.plot(huh_df['forecast'], label = 'SARIMAX Forecast')
plt.title('SARIMAX Forecast of Data')
plt.xlabel('Time')
plt.ylabel('Price')
plt.legend()

"""# **Feedforward Neural Network**"""

### FNN ###
model = Sequential()
model.add(Dense(40, input_dim = 2, activation='relu'))
model.add(Dense(400, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dense(300, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss = 'mse')
record = History()

input_matrix = corrected.drop(columns = ['Timestamp', 'Weighted_Price']).values
y_vector = corrected['Weighted_Price'].values

x_train, x_test, y_train, y_test = train_test_split(input_matrix, y_vector, test_size = 0.3, random_state = 42)

SCALER = StandardScaler()
SCALER.fit(x_train)
x_train = SCALER.transform(x_train)
x_test = SCALER.transform(x_test)

model.fit(x = x_train, y = y_train,validation_split=0.25, epochs = 1000, callbacks=[record])
preds = model.predict(x_test)
r2_score(y_test, preds)

plt.figure(figsize = (15,10))
plt.plot(record.history['loss'], label = 'Training Loss')
plt.plot(record.history['val_loss'], label = 'Validation Loss')
plt.title('Training and Validation Loss for FNN Without Sentiment')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

print('The minimum MSE is ' + str(min(record.history['loss'])))
print('The minimum Validation Loss is ' + str(min(record.history['val_loss'])))

"""# **Feedforward Neural Network With Sentiment Feature**"""

### FNN without Sentiment Feature ###
model = Sequential()
model.add(Dense(40, input_dim = 3, activation='relu'))
model.add(Dense(400, activation='relu'))
model.add(Dense(200, activation='relu'))
model.add(Dense(300, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss = 'mse')
record1 = History()

input_matrix = final_df.drop(columns = ['Timestamp', 'Weighted_Price']).values
y_vector = final_df['Weighted_Price'].values

x_train, x_test, y_train, y_test = train_test_split(input_matrix, y_vector, test_size = 0.3, random_state = 42)
SCALER = StandardScaler()
SCALER.fit(x_train)
x_train = SCALER.transform(x_train)
x_test = SCALER.transform(x_test)

model.fit(x = x_train, y = y_train,validation_split=0.25, epochs = 1000, callbacks=[record1])
preds = model.predict(x_test)
r2_score(y_test, preds)

plt.figure(figsize = (15,10))
plt.plot(record1.history['loss'], label = 'Training Loss')
plt.plot(record1.history['val_loss'], label = 'Validation Loss')
plt.title('Training and Validation Loss for FNN With Sentiment')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

print('The minimum MSE is ' + str(min(record1.history['loss'])))
print('The minimum Validation Loss is ' + str(min(record1.history['val_loss'])))